{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988fef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%config Completer.use_jedi = False\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cd87e",
   "metadata": {},
   "source": [
    "로지스틱 회귀는 참과 거짓 중에 하나를 내놓는 과정으로 참과 거짓을 구분한 'S'자를 눕혀놓은 형태의 선을 그어주는 작업이다.\n",
    "\n",
    "로지스틱 회귀분석 (지도학습 / 분류)\n",
    "\n",
    "- 종속변수(Y)가 범주형인 경우에 사용, 이진분류(0 아니면 1로 분류)\n",
    "- 시그모이드 함수 (S자형 곡선) : Y가 1일 확률값을 구해준다(0~1사이 값)\n",
    "- 임계값은 보통 p = 0.5,\n",
    "- 확률값이 0.5 보다 크면 1로 분류, 그렇지 않으면 0으로 분류\n",
    "\n",
    "Y가 Binary한 데이터로 주어질때는 Linear Regression을 사용하는 것 보다는 Logistic Regression을 사용하는 것이 훨씬 정확도가 높다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f445d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공부시간: [2, 4, 6, 8, 10, 12, 14], 합격여부: [0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 공부시간(x), 합격여부(y) => [공부시간, 합격여부]\n",
    "data = [[2, 0], [4, 0], [6, 0], [8, 1], [10, 1], [12, 1], [14, 1]]\n",
    "xData = [i[0] for i in data] # 공부시간 : 데이터 0행값들.\n",
    "yData = [i[1] for i in data] # 합격여부 \n",
    "print('공부시간: {}, 합격여부: {}'.format(xData, yData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c61e159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [0.64131213], b = [-1.38328264]\n"
     ]
    }
   ],
   "source": [
    "# 기울기(a)와 y절편(b) 값을 랜덤한 값으로 정한다.\n",
    "a = tf.Variable(tf.random_normal([1], dtype=tf.float64)) # 기울기(가중치)\n",
    "b = tf.Variable(tf.random_normal([1], dtype=tf.float64)) # y절편(바이어스)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('a = {}, b = {}'.format(sess.run(a), sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fbf90",
   "metadata": {},
   "source": [
    "시그모이드 방정식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b149a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "# np.e: 지수 값(2.718281828459045)을 의미하는 상수\n",
    "print(np.e)\n",
    "Y = 1 / (1 + np.e ** -(a * xData + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d0666",
   "metadata": {},
   "source": [
    "시그모이드 방정식의 오차를 계산하는 수식을 만든다.\n",
    "\n",
    "시그모이드 함수의 특성은 예측값(Y)이 항상 0아니면 1이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7fb565f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(np.array(yData) * tf.log(Y) + (1 - np.array(yData)) * tf.log(1 - Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495be64",
   "metadata": {},
   "source": [
    "오차를 최소로 하는 값을 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "850ea598",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1874f7",
   "metadata": {},
   "source": [
    "학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c0b74f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:      0, loss:   0.4564, a:   0.2097, b:  -0.6317\n",
      "epoch:   5000, loss:   0.0578, a:   1.5636, b: -10.7577\n",
      "epoch:  10000, loss:   0.0369, a:   2.0132, b: -13.9264\n",
      "epoch:  15000, loss:   0.0273, a:   2.3207, b: -16.0867\n",
      "epoch:  20000, loss:   0.0217, a:   2.5569, b: -17.7445\n",
      "epoch:  25000, loss:   0.0180, a:   2.7491, b: -19.0925\n",
      "epoch:  30000, loss:   0.0153, a:   2.9112, b: -20.2288\n",
      "epoch:  35000, loss:   0.0134, a:   3.0513, b: -21.2109\n",
      "epoch:  40000, loss:   0.0118, a:   3.1747, b: -22.0754\n",
      "epoch:  45000, loss:   0.0106, a:   3.2849, b: -22.8473\n",
      "epoch:  50000, loss:   0.0096, a:   3.3844, b: -23.5445\n",
      "epoch:  55000, loss:   0.0088, a:   3.4751, b: -24.1800\n",
      "epoch:  60000, loss:   0.0081, a:   3.5584, b: -24.7638\n",
      "epoch:  65000, loss:   0.0075, a:   3.6355, b: -25.3037\n",
      "epoch:  70000, loss:   0.0070, a:   3.7072, b: -25.8056\n",
      "epoch:  75000, loss:   0.0066, a:   3.7741, b: -26.2746\n",
      "epoch:  80000, loss:   0.0062, a:   3.8370, b: -26.7147\n",
      "epoch:  85000, loss:   0.0058, a:   3.8962, b: -27.1292\n",
      "epoch:  90000, loss:   0.0055, a:   3.9521, b: -27.5210\n",
      "epoch:  95000, loss:   0.0052, a:   4.0051, b: -27.8923\n",
      "epoch: 100000, loss:   0.0050, a:   4.0555, b: -28.2452\n",
      "epoch: 105000, loss:   0.0047, a:   4.1035, b: -28.5814\n",
      "epoch: 110000, loss:   0.0045, a:   4.1494, b: -28.9024\n",
      "epoch: 115000, loss:   0.0043, a:   4.1932, b: -29.2095\n",
      "epoch: 120000, loss:   0.0041, a:   4.2353, b: -29.5039\n",
      "epoch: 125000, loss:   0.0040, a:   4.2756, b: -29.7866\n",
      "epoch: 130000, loss:   0.0038, a:   4.3145, b: -30.0584\n",
      "epoch: 135000, loss:   0.0037, a:   4.3519, b: -30.3202\n",
      "epoch: 140000, loss:   0.0036, a:   4.3879, b: -30.5727\n",
      "epoch: 145000, loss:   0.0034, a:   4.4227, b: -30.8164\n",
      "epoch: 150000, loss:   0.0033, a:   4.4564, b: -31.0521\n",
      "epoch: 155000, loss:   0.0032, a:   4.4890, b: -31.2801\n",
      "epoch: 160000, loss:   0.0031, a:   4.5205, b: -31.5011\n",
      "epoch: 165000, loss:   0.0030, a:   4.5511, b: -31.7153\n",
      "epoch: 170000, loss:   0.0029, a:   4.5808, b: -31.9233\n",
      "epoch: 175000, loss:   0.0029, a:   4.6097, b: -32.1253\n",
      "epoch: 180000, loss:   0.0028, a:   4.6377, b: -32.3217\n",
      "epoch: 185000, loss:   0.0027, a:   4.6650, b: -32.5127\n",
      "epoch: 190000, loss:   0.0026, a:   4.6916, b: -32.6988\n",
      "epoch: 195000, loss:   0.0026, a:   4.7175, b: -32.8801\n",
      "epoch: 200000, loss:   0.0025, a:   4.7427, b: -33.0568\n",
      "epoch: 205000, loss:   0.0024, a:   4.7673, b: -33.2292\n",
      "epoch: 210000, loss:   0.0024, a:   4.7914, b: -33.3975\n",
      "epoch: 215000, loss:   0.0023, a:   4.8149, b: -33.5619\n",
      "epoch: 220000, loss:   0.0023, a:   4.8378, b: -33.7225\n",
      "epoch: 225000, loss:   0.0022, a:   4.8602, b: -33.8796\n",
      "epoch: 230000, loss:   0.0022, a:   4.8822, b: -34.0332\n",
      "epoch: 235000, loss:   0.0021, a:   4.9037, b: -34.1836\n",
      "epoch: 240000, loss:   0.0021, a:   4.9247, b: -34.3308\n",
      "epoch: 245000, loss:   0.0020, a:   4.9453, b: -34.4751\n",
      "epoch: 250000, loss:   0.0020, a:   4.9655, b: -34.6164\n",
      "epoch: 255000, loss:   0.0020, a:   4.9853, b: -34.7549\n",
      "epoch: 260000, loss:   0.0019, a:   5.0047, b: -34.8908\n",
      "epoch: 265000, loss:   0.0019, a:   5.0237, b: -35.0241\n",
      "epoch: 270000, loss:   0.0019, a:   5.0424, b: -35.1549\n",
      "epoch: 275000, loss:   0.0018, a:   5.0607, b: -35.2834\n",
      "epoch: 280000, loss:   0.0018, a:   5.0788, b: -35.4095\n",
      "epoch: 285000, loss:   0.0018, a:   5.0965, b: -35.5334\n",
      "epoch: 290000, loss:   0.0017, a:   5.1139, b: -35.6552\n",
      "epoch: 295000, loss:   0.0017, a:   5.1310, b: -35.7749\n",
      "epoch: 300000, loss:   0.0017, a:   5.1478, b: -35.8926\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(300001):\n",
    "    sess.run(gradient_descent)\n",
    "    if epoch % 5000 == 0:\n",
    "        # print('epoch: {:6d}, loss: {:8.4f}, a: {:8.4f}, b: {:8.4f}'.format(epoch, sess.run(loss), \n",
    "        #     sess.run(a)[0], sess.run(b)[0]))\n",
    "        print('epoch: %6d, loss: %8.4f, a: %8.4f, b: %8.4f' % (epoch, sess.run(loss), sess.run(a), sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1eec24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
